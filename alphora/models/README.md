# Alphora Models

**å¤§è¯­è¨€æ¨¡å‹ç»Ÿä¸€æ¥å£ç»„ä»¶**

Models æ˜¯ Alphora æ¡†æ¶çš„æ¨¡å‹æŠ½è±¡å±‚ï¼Œæä¾›ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£ã€‚å®ƒå…¼å®¹ OpenAI API è§„èŒƒï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥ã€æµå¼è¾“å‡ºã€è´Ÿè½½å‡è¡¡ã€å·¥å…·è°ƒç”¨ç­‰ç‰¹æ€§ï¼Œå¹¶é›†æˆè°ƒè¯•è¿½è¸ªèƒ½åŠ›ã€‚

## ç‰¹æ€§

- ğŸ”Œ **OpenAI å…¼å®¹** - å®Œå…¨å…¼å®¹ OpenAI Chat Completion API
- ğŸ–¼ **å¤šæ¨¡æ€æ”¯æŒ** - ç»Ÿä¸€çš„å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘æ¶ˆæ¯å¤„ç†
- ğŸ“¡ **åŒæ¨¡å¼è¾“å‡º** - æ”¯æŒæµå¼å’Œéæµå¼å“åº”
- âš–ï¸ **è´Ÿè½½å‡è¡¡** - å†…ç½®è½®è¯¢/éšæœºç­–ç•¥çš„å¤šåç«¯è´Ÿè½½å‡è¡¡
- ğŸ›  **å·¥å…·è°ƒç”¨** - å®Œæ•´çš„ Function Calling æ”¯æŒ
- ğŸ” **è°ƒè¯•è¿½è¸ª** - è¯·æ±‚/å“åº”/Token ç»Ÿè®¡è‡ªåŠ¨è¿½è¸ª
- ğŸ§© **å¯æ‰©å±•** - æ˜“äºæ‰©å±•æ”¯æŒæ–°çš„æ¨¡å‹æä¾›å•†
- ğŸ“Š **Embedding** - ç»Ÿä¸€çš„æ–‡æœ¬å‘é‡åŒ–æ¥å£

## å®‰è£…

```bash
pip install alphora
```

## å¿«é€Ÿå¼€å§‹

```python
from alphora.models import OpenAILike, Qwen

# åˆ›å»ºæ¨¡å‹å®ä¾‹
llm = OpenAILike(
    model_name="gpt-4",
    api_key="your-api-key",
    base_url="https://api.openai.com/v1"
)

# ç®€å•è°ƒç”¨
response = await llm.ainvoke("ä½ å¥½ï¼")
print(response)

# æµå¼è°ƒç”¨
generator = await llm.astream("å†™ä¸€é¦–è¯—")
async for chunk in generator:
    print(chunk.content, end="")
```

## ç›®å½•

- [åŸºç¡€ç”¨æ³•](#åŸºç¡€ç”¨æ³•)
- [æ¶ˆæ¯æ ¼å¼](#æ¶ˆæ¯æ ¼å¼)
- [æµå¼è¾“å‡º](#æµå¼è¾“å‡º)
- [å·¥å…·è°ƒç”¨](#å·¥å…·è°ƒç”¨)
- [è´Ÿè½½å‡è¡¡](#è´Ÿè½½å‡è¡¡)
- [æ¨¡å‹å˜ä½“](#æ¨¡å‹å˜ä½“)
- [æ–‡æœ¬å‘é‡åŒ–](#æ–‡æœ¬å‘é‡åŒ–)
- [è°ƒè¯•è¿½è¸ª](#è°ƒè¯•è¿½è¸ª)
- [API å‚è€ƒ](#api-å‚è€ƒ)

---

## åŸºç¡€ç”¨æ³•

### åˆ›å»ºæ¨¡å‹å®ä¾‹

```python
from alphora.models import OpenAILike

# åŸºç¡€é…ç½®
llm = OpenAILike(
    model_name="gpt-4",
    api_key="your-api-key",
    base_url="https://api.openai.com/v1"
)

# å®Œæ•´é…ç½®
llm = OpenAILike(
    model_name="gpt-4",
    api_key="your-api-key",
    base_url="https://api.openai.com/v1",
    temperature=0.7,
    max_tokens=2048,
    top_p=0.9,
    header={"X-Custom-Header": "value"},
    is_multimodal=True
)

# ä»ç¯å¢ƒå˜é‡è¯»å–
# è®¾ç½® LLM_API_KEY, LLM_BASE_URL, DEFAULT_LLM
llm = OpenAILike()
```

### åŒæ­¥è°ƒç”¨

```python
# ç®€å•è°ƒç”¨
response = llm.invoke("ä½ å¥½")
print(response)

# æµå¼è°ƒç”¨
generator = llm.stream("å†™ä¸€ç¯‡æ–‡ç« ")
for chunk in generator:
    print(chunk.content, end="")
```

### å¼‚æ­¥è°ƒç”¨

```python
# éæµå¼
response = await llm.ainvoke("ä½ å¥½")

# æµå¼ï¼ˆæ¨èï¼‰
generator = await llm.astream("å†™ä¸€ç¯‡æ–‡ç« ")
async for chunk in generator:
    print(chunk.content, end="")
```

### å‚æ•°è°ƒæ•´

```python
# è¿è¡Œæ—¶è°ƒæ•´
llm.set_temperature(0.8)
llm.set_max_tokens(4096)
llm.set_top_p(0.95)
llm.set_model_name("gpt-4-turbo")

# æ£€æŸ¥è¿æ¥
if await llm.aping():
    print("æ¨¡å‹è¿æ¥æ­£å¸¸")
```

---

## æ¶ˆæ¯æ ¼å¼

### Message ç±»

```python
from alphora.models.message import Message

# åˆ›å»ºæ–‡æœ¬æ¶ˆæ¯
msg = Message().add_text("è¿™æ˜¯ä¸€æ®µæ–‡å­—")

# åˆ›å»ºå¤šæ¨¡æ€æ¶ˆæ¯
msg = Message()
msg.add_text("è¯·æè¿°è¿™å¼ å›¾ç‰‡")
msg.add_image(base64_data, format="png")

# é“¾å¼è°ƒç”¨
msg = (Message()
    .add_text("åˆ†æä»¥ä¸‹å†…å®¹")
    .add_image(image_data)
    .add_audio(audio_data, format="mp3", duration=30.0))
```

### æ”¯æŒçš„åª’ä½“ç±»å‹

#### å›¾ç‰‡

```python
from alphora.models.message import Image

# æ”¯æŒæ ¼å¼ï¼špng, jpg, jpeg, bmp, dib, icns, jpeg2000, tiff
msg.add_image(
    data="base64ç¼–ç çš„å›¾ç‰‡æ•°æ®",
    format="png"
)

# è·å– DataURL
image = Image(data=base64_data, format="jpg")
print(image.data_url)  # data:image/jpg;base64,...
```

#### éŸ³é¢‘

```python
from alphora.models.message import Audio

# æ”¯æŒæ ¼å¼ï¼šmp3, wav, ogg, flac, aac, m4a
msg.add_audio(
    data="base64ç¼–ç çš„éŸ³é¢‘æ•°æ®",
    format="mp3",
    duration=60.0  # ç§’
)
```

#### è§†é¢‘

```python
from alphora.models.message import Video

# æ”¯æŒæ ¼å¼ï¼šmp4, webm, mov, avi, mkv, flv
msg.add_video(
    data="base64ç¼–ç çš„è§†é¢‘æ•°æ®",
    format="mp4",
    duration=120.0
)
```

### æ¶ˆæ¯æ£€æŸ¥

```python
msg = Message().add_text("ä½ å¥½").add_image(img_data)

# æ£€æŸ¥å†…å®¹ç±»å‹
msg.has_text()    # True
msg.has_images()  # True
msg.has_audios()  # False
msg.has_videos()  # False

# è½¬ä¸º OpenAI æ ¼å¼
openai_msg = msg.to_openai_format(role="user")
# {
#     "role": "user",
#     "content": [
#         {"type": "text", "text": "ä½ å¥½"},
#         {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}
#     ]
# }
```

### ä½¿ç”¨æ¶ˆæ¯è°ƒç”¨

```python
# åˆ›å»ºå¤šæ¨¡æ€æ¶ˆæ¯
msg = Message()
msg.add_text("è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ")
msg.add_image(image_base64, format="png")

# è°ƒç”¨ï¼ˆéœ€è¦å¤šæ¨¡æ€æ¨¡å‹ï¼‰
response = await llm.ainvoke(msg)

# æˆ–æµå¼
generator = await llm.astream(msg)
```

---

## æµå¼è¾“å‡º

### ç”Ÿæˆå™¨ç»“æ„

```python
from alphora.models.llms.stream_helper import GeneratorOutput

generator = await llm.astream("ä½ å¥½")

async for output in generator:
    content = output.content       # æ–‡æœ¬å†…å®¹
    content_type = output.content_type  # å†…å®¹ç±»å‹
    
    if content_type == "think":
        print(f"[æ€è€ƒ] {content}")
    else:
        print(content, end="")

# è·å–ç»“æŸåŸå› 
print(generator.finish_reason)  # stop, length, tool_calls
```

### å¯ç”¨æ€è€ƒæ¨¡å¼

```python
# éƒ¨åˆ†æ¨¡å‹æ”¯æŒï¼ˆå¦‚ Qwen3ï¼‰
generator = await llm.aget_streaming_response(
    message="å¤æ‚é—®é¢˜",
    enable_thinking=True
)

reasoning = ""
content = ""

async for output in generator:
    if output.content_type == "think":
        reasoning += output.content
    else:
        content += output.content
```

### è‡ªå®šä¹‰å†…å®¹ç±»å‹

```python
generator = await llm.aget_streaming_response(
    message="ç”ŸæˆSQLæŸ¥è¯¢",
    content_type="sql"  # æ ‡è®°è¾“å‡ºç±»å‹
)
```

---

## å·¥å…·è°ƒç”¨

### éæµå¼å·¥å…·è°ƒç”¨

```python
from alphora.models.llms.types import ToolCall

# å®šä¹‰å·¥å…·
tools = [{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "è·å–å¤©æ°”ä¿¡æ¯",
        "parameters": {
            "type": "object",
            "properties": {
                "city": {"type": "string", "description": "åŸå¸‚å"}
            },
            "required": ["city"]
        }
    }
}]

# è°ƒç”¨
response = await llm.aget_non_stream_response(
    message="åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
    tools=tools
)

# æ£€æŸ¥å·¥å…·è°ƒç”¨
if isinstance(response, ToolCall) and response.has_tool_calls:
    for tc in response.tool_calls:
        print(tc["function"]["name"])
        print(tc["function"]["arguments"])
```

### æµå¼å·¥å…·è°ƒç”¨

```python
generator = await llm.aget_streaming_response(
    message="æŸ¥è¯¢å¤©æ°”",
    tools=tools
)

# æ¶ˆè´¹æµ
async for chunk in generator:
    if chunk.content:
        print(chunk.content, end="")

# æµç»“æŸåè·å–å·¥å…·è°ƒç”¨
collected_tools = generator.collected_tool_calls
if collected_tools:
    tool_call = ToolCall(tool_calls=collected_tools)
    tool_call.pretty_print()
```

### ToolCall å¯¹è±¡

```python
from alphora.models.llms.types import ToolCall

# ToolCall ç»§æ‰¿è‡ª listï¼Œå¯è¿­ä»£
for tc in tool_call:
    print(tc)

# å±æ€§å’Œæ–¹æ³•
tool_call.has_tool_calls      # æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
tool_call.content             # æ–‡æœ¬å†…å®¹ï¼ˆå¯èƒ½ä¸º Noneï¼‰
tool_call.tool_calls          # å·¥å…·è°ƒç”¨åˆ—è¡¨
tool_call.get_tool_names()    # ['get_weather', 'search']
tool_call.get_tool_call_ids() # ['call_abc', 'call_def']

# æ ¼å¼åŒ–è¾“å‡º
tool_call.pretty_print()
# ğŸ”§ å·¥å…·è°ƒç”¨è¯¦æƒ… (å…± 1 ä¸ª)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# [1] get_weather
#     ID: call_abc123
#     å‚æ•°:
#       â€¢ city: "åŒ—äº¬"

print(tool_call.to_summary())
# è°ƒç”¨ 1 ä¸ªå·¥å…·: get_weather(city="åŒ—äº¬")
```

---

## è´Ÿè½½å‡è¡¡

### æ·»åŠ å¤šä¸ªåç«¯

```python
from alphora.models import OpenAILike

# ä¸»æ¨¡å‹
llm = OpenAILike(
    model_name="gpt-4",
    api_key="key1",
    base_url="https://api1.example.com/v1"
)

# æ·»åŠ å¤‡ç”¨æ¨¡å‹ï¼ˆä½¿ç”¨ + è¿ç®—ç¬¦ï¼‰
backup = OpenAILike(
    model_name="gpt-4",
    api_key="key2",
    base_url="https://api2.example.com/v1"
)

llm = llm + backup  # è‡ªåŠ¨è´Ÿè½½å‡è¡¡

# è°ƒç”¨æ—¶è‡ªåŠ¨è½®è¯¢
response = await llm.ainvoke("ä½ å¥½")
```

### å¤šæ¨¡æ€è´Ÿè½½å‡è¡¡

```python
# æ ‡è®°å¤šæ¨¡æ€æ”¯æŒ
llm_text = OpenAILike(model_name="gpt-4", is_multimodal=False)
llm_vision = OpenAILike(model_name="gpt-4-vision", is_multimodal=True)

llm = llm_text + llm_vision

# æ–‡æœ¬è¯·æ±‚ - ä¸¤ä¸ªåç«¯éƒ½å¯ç”¨
response = await llm.ainvoke("ä½ å¥½")

# å¤šæ¨¡æ€è¯·æ±‚ - åªä½¿ç”¨æ”¯æŒçš„åç«¯
msg = Message().add_text("æè¿°å›¾ç‰‡").add_image(img_data)
response = await llm.ainvoke(msg)  # è‡ªåŠ¨è·¯ç”±åˆ° llm_vision
```

### è´Ÿè½½å‡è¡¡ç­–ç•¥

```python
from alphora.models.llms.balancer import _LLMLoadBalancer

# è½®è¯¢ï¼ˆé»˜è®¤ï¼‰
balancer = _LLMLoadBalancer(strategy="round_robin")

# éšæœº
balancer = _LLMLoadBalancer(strategy="random")
```

---

## æ¨¡å‹å˜ä½“

### Qwenï¼ˆé€šä¹‰åƒé—®ï¼‰

```python
from alphora.models import Qwen

# ä½¿ç”¨ DashScope API
llm = Qwen(
    model_name="qwen-max",  # qwen-max, qwen-plus, qwen-turbo, qwen3-32b
    api_key="your-dashscope-key",
    temperature=0.7
)

# Qwen3 ç³»åˆ—æ”¯æŒæ€è€ƒæ¨¡å¼
llm = Qwen(model_name="qwen3-32b")
generator = await llm.astream(
    "å¤æ‚æ¨ç†é—®é¢˜",
    enable_thinking=True
)
```

### DeepSeek

```python
from alphora.models.llms.deepseek import DeepSeek

llm = DeepSeek(
    model_name="deepseek-chat",
    api_key="your-deepseek-key"
)
```

### è‡ªå®šä¹‰æ¨¡å‹

```python
from alphora.models.llms.openai_like import OpenAILike

# ä»»ä½• OpenAI å…¼å®¹çš„ API
llm = OpenAILike(
    model_name="custom-model",
    api_key="your-key",
    base_url="https://your-api.com/v1"
)

# ç»§æ‰¿å®ç°è‡ªå®šä¹‰é€»è¾‘
class MyModel(OpenAILike):
    def _get_extra_body(self, enable_thinking=False):
        return {"custom_param": "value"}
```

---

## æ–‡æœ¬å‘é‡åŒ–

### åŸºç¡€ä½¿ç”¨

```python
from alphora.models.embedder import EmbeddingModel

embedder = EmbeddingModel(
    model="text-embedding-v3",
    api_key="your-key",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# å•æ–‡æœ¬
embedding = embedder.get_text_embedding("ä½ å¥½ä¸–ç•Œ")
# [0.123, -0.456, ...]

# æ‰¹é‡ï¼ˆè‡ªåŠ¨åˆ†æ‰¹ï¼‰
embeddings = embedder.get_text_embeddings(
    ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"],
    max_batch=10
)
```

### å¼‚æ­¥è°ƒç”¨

```python
# å•æ–‡æœ¬
embedding = await embedder.aget_text_embedding("ä½ å¥½")

# æ‰¹é‡
embeddings = await embedder.aget_text_embeddings(["æ–‡æœ¬1", "æ–‡æœ¬2"])
```

### è¿æ¥æ£€æŸ¥

```python
# åŒæ­¥
if embedder.ping():
    print("Embedding æœåŠ¡æ­£å¸¸")

# å¼‚æ­¥
if await embedder.aping():
    print("Embedding æœåŠ¡æ­£å¸¸")
```

---

## è°ƒè¯•è¿½è¸ª

### è‡ªåŠ¨è¿½è¸ª

OpenAILike ä¼šè‡ªåŠ¨è¿½è¸ªä»¥ä¸‹å†…å®¹ï¼š

- è¯·æ±‚å‚æ•°ï¼ˆmodel, temperature, max_tokensï¼‰
- è¾“å…¥æ¶ˆæ¯
- è¾“å‡ºå†…å®¹
- Token ç»Ÿè®¡
- è€—æ—¶
- é”™è¯¯ä¿¡æ¯

### å…³è” Agent

```python
# è®¾ç½® Agent ID ä»¥å…³è”è¿½è¸ª
llm.agent_id = "my_agent"

# è¿½è¸ªä¿¡æ¯ä¼šåŒ…å« agent_id
response = await llm.ainvoke("ä½ å¥½")
```

### æŸ¥çœ‹è¿½è¸ª

```python
from alphora.debugger import tracer

# å¯ç”¨è°ƒè¯•å™¨
tracer.enable(start_server=True, port=9527)

# è®¿é—® http://localhost:9527 æŸ¥çœ‹è°ƒè¯•ç•Œé¢
```

---

## API å‚è€ƒ

### OpenAILike

#### æ„é€ å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `model_name` | `str` | ç¯å¢ƒå˜é‡ | æ¨¡å‹åç§° |
| `api_key` | `str` | ç¯å¢ƒå˜é‡ | API å¯†é’¥ |
| `base_url` | `str` | ç¯å¢ƒå˜é‡ | API åŸºç¡€ URL |
| `header` | `Mapping` | `None` | é¢å¤–è¯·æ±‚å¤´ |
| `temperature` | `float` | `0.0` | é‡‡æ ·æ¸©åº¦ |
| `max_tokens` | `int` | `1024` | æœ€å¤§ç”Ÿæˆ Token |
| `top_p` | `float` | `1.0` | æ ¸é‡‡æ ·å‚æ•° |
| `is_multimodal` | `bool` | `False` | æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€ |

#### æ–¹æ³•

| æ–¹æ³• | è¯´æ˜ |
|------|------|
| `invoke(message)` | åŒæ­¥éæµå¼è°ƒç”¨ |
| `ainvoke(message)` | å¼‚æ­¥éæµå¼è°ƒç”¨ |
| `stream(message, ...)` | åŒæ­¥æµå¼è°ƒç”¨ |
| `astream(message, ...)` | å¼‚æ­¥æµå¼è°ƒç”¨ |
| `get_non_stream_response(message, tools, ...)` | åº•å±‚éæµå¼æ–¹æ³• |
| `aget_non_stream_response(message, tools, ...)` | åº•å±‚å¼‚æ­¥éæµå¼æ–¹æ³• |
| `get_streaming_response(message, ...)` | åº•å±‚æµå¼æ–¹æ³• |
| `aget_streaming_response(message, ...)` | åº•å±‚å¼‚æ­¥æµå¼æ–¹æ³• |
| `set_temperature(temp)` | è®¾ç½®æ¸©åº¦ |
| `set_max_tokens(tokens)` | è®¾ç½®æœ€å¤§ Token |
| `set_top_p(p)` | è®¾ç½® top_p |
| `set_model_name(name)` | è®¾ç½®æ¨¡å‹å |
| `ping()` | åŒæ­¥è¿æ¥æ£€æŸ¥ |
| `aping()` | å¼‚æ­¥è¿æ¥æ£€æŸ¥ |

### Message

#### æ–¹æ³•

| æ–¹æ³• | è¯´æ˜ |
|------|------|
| `add_text(content)` | æ·»åŠ æ–‡æœ¬ |
| `add_image(data, format)` | æ·»åŠ å›¾ç‰‡ |
| `add_audio(data, format, duration)` | æ·»åŠ éŸ³é¢‘ |
| `add_video(data, format, duration)` | æ·»åŠ è§†é¢‘ |
| `add_function_call(name, parameters)` | æ·»åŠ å‡½æ•°è°ƒç”¨ |
| `add_function_result(name, result, success, error)` | æ·»åŠ å‡½æ•°ç»“æœ |
| `has_text()` | æ˜¯å¦æœ‰æ–‡æœ¬ |
| `has_images()` | æ˜¯å¦æœ‰å›¾ç‰‡ |
| `has_audios()` | æ˜¯å¦æœ‰éŸ³é¢‘ |
| `has_videos()` | æ˜¯å¦æœ‰è§†é¢‘ |
| `to_openai_format(role)` | è½¬ä¸º OpenAI æ ¼å¼ |

### EmbeddingModel

#### æ„é€ å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `model` | `str` | `'text-embedding-v3'` | æ¨¡å‹åç§° |
| `api_key` | `str` | ç¯å¢ƒå˜é‡ | API å¯†é’¥ |
| `base_url` | `str` | DashScope | API åœ°å€ |
| `dimension` | `int` | `None` | å‘é‡ç»´åº¦ |
| `header` | `dict` | `None` | é¢å¤–è¯·æ±‚å¤´ |

#### æ–¹æ³•

| æ–¹æ³• | è¯´æ˜ |
|------|------|
| `get_text_embedding(text)` | è·å–å•æ–‡æœ¬å‘é‡ |
| `get_text_embeddings(texts, max_batch)` | æ‰¹é‡è·å–å‘é‡ |
| `aget_text_embedding(text)` | å¼‚æ­¥è·å–å•æ–‡æœ¬å‘é‡ |
| `aget_text_embeddings(texts, max_batch)` | å¼‚æ­¥æ‰¹é‡è·å–å‘é‡ |
| `get_embedding_dimension()` | è·å–å‘é‡ç»´åº¦ |
| `ping()` | åŒæ­¥è¿æ¥æ£€æŸ¥ |
| `aping()` | å¼‚æ­¥è¿æ¥æ£€æŸ¥ |

### GeneratorOutput

| å±æ€§ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `content` | `str` | æ–‡æœ¬å†…å®¹ |
| `content_type` | `str` | å†…å®¹ç±»å‹ï¼ˆchar/think/ç­‰ï¼‰ |

### ToolCall

| å±æ€§/æ–¹æ³• | ç±»å‹ | è¯´æ˜ |
|-----------|------|------|
| `tool_calls` | `List[Dict]` | å·¥å…·è°ƒç”¨åˆ—è¡¨ |
| `content` | `str \| None` | æ–‡æœ¬å†…å®¹ |
| `has_tool_calls` | `bool` | æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ |
| `get_tool_names()` | `List[str]` | è·å–å·¥å…·åç§°åˆ—è¡¨ |
| `get_tool_call_ids()` | `List[str]` | è·å–è°ƒç”¨ ID åˆ—è¡¨ |
| `format_details(indent)` | `str` | æ ¼å¼åŒ–è¯¦æƒ…å­—ç¬¦ä¸² |
| `pretty_print(indent)` | `None` | æ‰“å°æ ¼å¼åŒ–è¯¦æƒ… |
| `to_summary()` | `str` | ç”Ÿæˆå•è¡Œæ‘˜è¦ |