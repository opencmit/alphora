# Alphora Prompter

**æ™ºèƒ½ä½“æç¤ºè¯ç¼–æ’å¼•æ“**

Prompter æ˜¯ Alphora æ¡†æ¶çš„æ ¸å¿ƒæç¤ºè¯ç¼–æ’ç»„ä»¶ï¼Œæä¾›åŸºäº Jinja2 çš„åŠ¨æ€æ¨¡æ¿æ¸²æŸ“ã€LLM ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€æµå¼è¾“å‡ºå¤„ç†ç­‰èƒ½åŠ›ã€‚å®ƒæ”¯æŒå·¥å…·è°ƒç”¨ã€é•¿æ–‡æœ¬ç»­å†™ã€å¹¶è¡Œæ‰§è¡Œç­‰é«˜çº§ç‰¹æ€§ï¼Œæ˜¯æ„å»ºå¤æ‚ LLM åº”ç”¨çš„åŸºç¡€è®¾æ–½ã€‚

## ç‰¹æ€§

- ğŸ¯ **Jinja2 æ¨¡æ¿** - å¼ºå¤§çš„åŠ¨æ€æ¨¡æ¿æ¸²æŸ“ï¼Œæ”¯æŒå˜é‡æ’å€¼å’Œé€»è¾‘æ§åˆ¶
- ğŸ“ **åŒæ¨¡å¼è°ƒç”¨** - å®Œæ•´æ”¯æŒåŒæ­¥ `call` å’Œå¼‚æ­¥ `acall` è°ƒç”¨æ¨¡å¼
- ğŸ”„ **æµå¼è¾“å‡º** - åŸç”Ÿæ”¯æŒ Token çº§æµå¼å“åº”å’Œå›è°ƒå¤„ç†
- ğŸ›  **å·¥å…·è°ƒç”¨** - æ— ç¼é›†æˆ Function Callingï¼Œæ”¯æŒæµå¼å·¥å…·è°ƒç”¨
- ğŸ“š **å†å²é›†æˆ** - ä¸ MemoryManager æ·±åº¦é›†æˆï¼Œè§„èŒƒåŒ–æ¶ˆæ¯åºåˆ—
- âœ¨ **é•¿æ–‡æœ¬ç»­å†™** - è‡ªåŠ¨æ£€æµ‹æˆªæ–­å¹¶ç»­å†™ï¼Œçªç ´ Token é™åˆ¶
- âš¡ **å¹¶è¡Œæ‰§è¡Œ** - æ”¯æŒå¤šæç¤ºè¯å¹¶è¡Œè°ƒç”¨ï¼Œæå‡ååé‡
- ğŸ”§ **åå¤„ç†å™¨** - çµæ´»çš„æµå¼è¾“å‡ºåå¤„ç†ç®¡é“

## å®‰è£…

```bash
pip install alphora
```

## å¿«é€Ÿå¼€å§‹

```python
from alphora.prompter import BasePrompt
from alphora.models import OpenAILike

# åˆ›å»ºæç¤ºè¯
prompt = BasePrompt(
    system_prompt="ä½ æ˜¯ä¸€ä¸ªPythonä¸“å®¶",
    user_prompt="è¯·è§£é‡Šï¼š{{query}}"
)

# ç»‘å®š LLM
prompt.add_llm(OpenAILike(model_name="gpt-4"))

# è°ƒç”¨
response = await prompt.acall(query="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ", is_stream=True)
```

## ç›®å½•

- [åŸºç¡€ç”¨æ³•](#åŸºç¡€ç”¨æ³•)
- [æ¨¡æ¿ç³»ç»Ÿ](#æ¨¡æ¿ç³»ç»Ÿ)
- [æ¶ˆæ¯æ„å»º](#æ¶ˆæ¯æ„å»º)
- [æµå¼è¾“å‡º](#æµå¼è¾“å‡º)
- [å·¥å…·è°ƒç”¨](#å·¥å…·è°ƒç”¨)
- [é•¿æ–‡æœ¬ç»­å†™](#é•¿æ–‡æœ¬ç»­å†™)
- [å¹¶è¡Œæ‰§è¡Œ](#å¹¶è¡Œæ‰§è¡Œ)
- [åå¤„ç†å™¨](#åå¤„ç†å™¨)
- [API å‚è€ƒ](#api-å‚è€ƒ)

---

## åŸºç¡€ç”¨æ³•

### åˆ›å»ºæç¤ºè¯

```python
from alphora.prompter import BasePrompt

# æ–¹å¼ 1ï¼šç›´æ¥ä¼ å…¥å­—ç¬¦ä¸²
prompt = BasePrompt(
    user_prompt="è¯·å›ç­”ï¼š{{query}}"
)

# æ–¹å¼ 2ï¼šå¸¦ç³»ç»Ÿæç¤º
prompt = BasePrompt(
    system_prompt="ä½ æ˜¯ä¸€ä¸ª{{role}}åŠ©æ‰‹",
    user_prompt="{{query}}"
)

# æ–¹å¼ 3ï¼šä»æ–‡ä»¶åŠ è½½
prompt = BasePrompt(
    template_path="prompts/qa.txt"
)

# æ–¹å¼ 4ï¼šå¤šæ®µç³»ç»Ÿæç¤º
prompt = BasePrompt(
    system_prompt=[
        "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹",
        "è¯·ç”¨ç®€æ´çš„è¯­è¨€å›ç­”",
        "å¦‚æœä¸ç¡®å®šï¼Œè¯·è¯´æ˜"
    ],
    user_prompt="{{query}}"
)
```

### ç»‘å®š LLM

```python
from alphora.models import OpenAILike, Qwen

# ç»‘å®š OpenAI å…¼å®¹æ¨¡å‹
prompt.add_llm(OpenAILike(model_name="gpt-4"))

# æˆ– Qwen æ¨¡å‹
prompt.add_llm(Qwen(model_name="qwen-max"))

# é“¾å¼è°ƒç”¨
prompt = BasePrompt(user_prompt="{{query}}").add_llm(llm)
```

### æ›´æ–°å ä½ç¬¦

```python
prompt = BasePrompt(
    system_prompt="ä½ æ˜¯{{company}}çš„{{role}}",
    user_prompt="{{context}}\n\né—®é¢˜ï¼š{{query}}"
)

# æ›´æ–°å˜é‡
prompt.update_placeholder(
    company="Anthropic",
    role="AIåŠ©æ‰‹",
    context="ä»¥ä¸‹æ˜¯èƒŒæ™¯ä¿¡æ¯..."
)

# é“¾å¼è°ƒç”¨
prompt.update_placeholder(company="OpenAI").update_placeholder(role="ä¸“å®¶")

# æŸ¥çœ‹å¯ç”¨å ä½ç¬¦
print(prompt.placeholders)  # ['company', 'role', 'context']
```

### è°ƒç”¨æ–¹å¼

```python
# å¼‚æ­¥è°ƒç”¨ï¼ˆæ¨èï¼‰
response = await prompt.acall(query="ä½ å¥½")

# åŒæ­¥è°ƒç”¨
response = prompt.call(query="ä½ å¥½")

# æµå¼è°ƒç”¨
response = await prompt.acall(query="å†™ä¸€ç¯‡æ–‡ç« ", is_stream=True)

# éæµå¼è°ƒç”¨
response = await prompt.acall(query="ç®€å•é—®é¢˜", is_stream=False)
```

---

## æ¨¡æ¿ç³»ç»Ÿ

### Jinja2 è¯­æ³•

```python
# å˜é‡æ’å€¼
prompt = BasePrompt(user_prompt="ä½ å¥½ï¼Œ{{name}}ï¼")

# æ¡ä»¶åˆ¤æ–­
prompt = BasePrompt(user_prompt="""
{% if detailed %}
è¯·è¯¦ç»†è§£é‡Šï¼š{{query}}
{% else %}
ç®€è¦å›ç­”ï¼š{{query}}
{% endif %}
""")

# å¾ªç¯
prompt = BasePrompt(user_prompt="""
è¯·åˆ†æä»¥ä¸‹é¡¹ç›®ï¼š
{% for item in items %}
- {{item}}
{% endfor %}
""")
```

### æ¨¡æ¿æ–‡ä»¶

```python
# prompts/analysis.txt
"""
ä½ éœ€è¦åˆ†æä»¥ä¸‹æ•°æ®ï¼š
{{data}}

åˆ†æè§’åº¦ï¼š
{% for angle in angles %}
{{loop.index}}. {{angle}}
{% endfor %}

è¾“å‡ºæ ¼å¼ï¼š{{format}}
"""

prompt = BasePrompt(template_path="prompts/analysis.txt")
prompt.update_placeholder(
    data="é”€å”®æ•°æ®...",
    angles=["è¶‹åŠ¿", "å¼‚å¸¸", "é¢„æµ‹"],
    format="JSON"
)
```

### æ¸²æŸ“é¢„è§ˆ

```python
# æŸ¥çœ‹æ¸²æŸ“ç»“æœ
print(prompt.render())

# å®Œæ•´é¢„è§ˆï¼ˆåŒ…å« system å’Œ userï¼‰
print(prompt)
# [System Prompts]
#  - ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹
# [User Prompt]
# è¯·å›ç­”ï¼š{{query}}
```

---

## æ¶ˆæ¯æ„å»º

### build_messages æ–¹æ³•

```python
# æ„å»ºæ ‡å‡†æ¶ˆæ¯åˆ—è¡¨
messages = prompt.build_messages(
    query="ä½ å¥½",
    force_json=False,
    runtime_system_prompt=None,
    history=None
)
# [
#     {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹"},
#     {"role": "user", "content": "ä½ å¥½"}
# ]
```

### æ¶ˆæ¯é¡ºåº

æ¶ˆæ¯æŒ‰ä»¥ä¸‹é¡ºåºç»„è£…ï¼š

1. **JSON çº¦æŸ**ï¼ˆå¦‚æœ `force_json=True`ï¼‰
2. **é¢„è®¾ System Prompts**
3. **è¿è¡Œæ—¶ System Prompts**
4. **å†å²è®°å½•**ï¼ˆæ¥è‡ª HistoryPayloadï¼‰
5. **User è¾“å…¥**

### ä¸ Memory é›†æˆ

```python
from alphora.memory import MemoryManager

memory = MemoryManager()
memory.add_user("ä¹‹å‰çš„é—®é¢˜")
memory.add_assistant("ä¹‹å‰çš„å›ç­”")

# æ„å»ºå¸¦å†å²çš„æ¶ˆæ¯
history = memory.build_history()
response = await prompt.acall(
    query="ç»§ç»­ä¸Šé¢çš„è¯é¢˜",
    history=history
)
```

### è¿è¡Œæ—¶ç³»ç»Ÿæç¤º

```python
# åŠ¨æ€æ·»åŠ ç³»ç»ŸæŒ‡ä»¤
response = await prompt.acall(
    query="åˆ†ææ•°æ®",
    runtime_system_prompt="è¯·ç”¨è¡¨æ ¼å½¢å¼è¾“å‡º"
)

# å¤šæ¡è¿è¡Œæ—¶æŒ‡ä»¤
response = await prompt.acall(
    query="åˆ†ææ•°æ®",
    runtime_system_prompt=[
        "è¯·ç”¨è¡¨æ ¼å½¢å¼è¾“å‡º",
        "åŒ…å«æ•°æ®æ¥æºè¯´æ˜"
    ]
)
```

---

## æµå¼è¾“å‡º

### åŸºç¡€æµå¼

```python
# æµå¼è°ƒç”¨ï¼Œè‡ªåŠ¨æ‰“å°
response = await prompt.acall(
    query="å†™ä¸€é¦–è¯—",
    is_stream=True
)

# å¸¦å›è°ƒçš„æµå¼
prompt = BasePrompt(
    user_prompt="{{query}}",
    callback=data_streamer  # è‡ªåŠ¨æ¨é€åˆ°å®¢æˆ·ç«¯
)
```

### è·å–ç”Ÿæˆå™¨

```python
# è·å–åŸå§‹ç”Ÿæˆå™¨ä»¥è‡ªå®šä¹‰å¤„ç†
generator = await prompt.acall(
    query="å†™æ–‡ç« ",
    is_stream=True,
    return_generator=True
)

# è‡ªå®šä¹‰æ¶ˆè´¹
async for chunk in generator:
    print(chunk.content, end="")
    # chunk.content_type: 'char', 'think', ç­‰
```

### å¯ç”¨æ€è€ƒé“¾

```python
response = await prompt.acall(
    query="å¤æ‚æ¨ç†é—®é¢˜",
    is_stream=True,
    enable_thinking=True
)

# è®¿é—®æ¨ç†å†…å®¹
print(response.reasoning)  # æ€è€ƒè¿‡ç¨‹
print(response)            # æœ€ç»ˆå›ç­”
```

### å†…å®¹ç±»å‹

```python
response = await prompt.acall(
    query="ç”ŸæˆSQL",
    is_stream=True,
    content_type="sql"  # æŒ‡å®šè¾“å‡ºç±»å‹
)
```

---

## å·¥å…·è°ƒç”¨

### åŸºç¡€å·¥å…·è°ƒç”¨

```python
from alphora.tools.decorators import tool

@tool
def get_weather(city: str) -> str:
    """è·å–å¤©æ°”ä¿¡æ¯"""
    return f"{city}: æ™´, 25Â°C"

tools_schema = [get_weather.to_openai_schema()]

# éæµå¼å·¥å…·è°ƒç”¨
response = await prompt.acall(
    query="åŒ—äº¬å¤©æ°”å¦‚ä½•ï¼Ÿ",
    tools=tools_schema
)

if response.has_tool_calls:
    print(response.tool_calls)  # å·¥å…·è°ƒç”¨åˆ—è¡¨
else:
    print(response.content)     # æ–‡æœ¬å“åº”
```

### æµå¼å·¥å…·è°ƒç”¨

```python
# æµå¼ä¹Ÿæ”¯æŒå·¥å…·è°ƒç”¨
response = await prompt.acall(
    query="æŸ¥è¯¢å¤©æ°”",
    is_stream=True,
    tools=tools_schema
)

# ToolCall å¯¹è±¡
if response.has_tool_calls:
    response.pretty_print()  # æ ¼å¼åŒ–æ˜¾ç¤º
    print(response.to_summary())  # ç®€çŸ­æ‘˜è¦
```

### å®Œæ•´å·¥å…·é“¾

```python
from alphora.memory import MemoryManager

memory = MemoryManager()
memory.add_user("åŒ—äº¬å¤©æ°”ï¼Ÿ")

# ç¬¬ä¸€è½®ï¼šè·å–å·¥å…·è°ƒç”¨
history = memory.build_history()
response = await prompt.acall(history=history, tools=tools_schema)

if response.has_tool_calls:
    memory.add_assistant(response)
    
    # æ‰§è¡Œå·¥å…·
    results = await executor.execute(response)
    memory.add_tool_result(results)
    
    # ç¬¬äºŒè½®ï¼šç”Ÿæˆæœ€ç»ˆå›ç­”
    history = memory.build_history()
    final = await prompt.acall(history=history)
    memory.add_assistant(final)
```

---

## é•¿æ–‡æœ¬ç»­å†™

å½“æ¨¡å‹è¾“å‡ºå›  Token é™åˆ¶è¢«æˆªæ–­æ—¶ï¼Œè‡ªåŠ¨ç»­å†™ç”Ÿæˆå®Œæ•´å†…å®¹ã€‚

### å¯ç”¨é•¿æ–‡æœ¬æ¨¡å¼

```python
response = await prompt.acall(
    query="å†™ä¸€ç¯‡10000å­—çš„å°è¯´",
    is_stream=True,
    long_response=True  # å¯ç”¨è‡ªåŠ¨ç»­å†™
)

# æŸ¥çœ‹ç»­å†™æ¬¡æ•°
print(response.continuation_count)  # ç»­å†™äº†å‡ æ¬¡
```

### å·¥ä½œåŸç†

1. æ£€æµ‹ `finish_reason == 'length'`ï¼ˆToken è€—å°½ï¼‰
2. è‡ªåŠ¨æ„å»ºç»­å†™æç¤ºï¼ŒåŒ…å«ï¼š
    - åŸå§‹ä»»åŠ¡æè¿°
    - å·²ç”Ÿæˆå†…å®¹çš„å°¾éƒ¨ï¼ˆä¸Šä¸‹æ–‡ï¼‰
    - ç»­å†™æŒ‡ä»¤
3. å¾ªç¯ç”Ÿæˆç›´åˆ°è‡ªç„¶ç»“æŸæˆ–è¾¾åˆ°æœ€å¤§æ¬¡æ•°

### é…ç½®å‚æ•°

```python
from alphora.prompter.long_response import LongResponseGenerator

generator = LongResponseGenerator(
    llm=llm,
    original_message=message,
    content_type="char",
    enable_thinking=False
)

# å†…éƒ¨å‚æ•°
generator.max_continuations = 100  # æœ€å¤§ç»­å†™æ¬¡æ•°
generator.tail_length = 1500       # ä¸Šä¸‹æ–‡å°¾éƒ¨é•¿åº¦
generator.min_chunk_length = 50    # æœ€å°è¾“å‡ºé•¿åº¦ï¼ˆé˜²æ­¢ç©ºå¾ªç¯ï¼‰
```

---

## å¹¶è¡Œæ‰§è¡Œ

### ç®¡é“è¿ç®—ç¬¦

```python
from alphora.prompter import BasePrompt

prompt1 = BasePrompt(user_prompt="ç¿»è¯‘æˆè‹±æ–‡ï¼š{{query}}")
prompt2 = BasePrompt(user_prompt="ç¿»è¯‘æˆæ—¥æ–‡ï¼š{{query}}")
prompt3 = BasePrompt(user_prompt="ç¿»è¯‘æˆæ³•æ–‡ï¼š{{query}}")

# ä½¿ç”¨ | è¿ç®—ç¬¦ç»„åˆ
parallel_prompt = prompt1 | prompt2 | prompt3

# å¹¶è¡Œæ‰§è¡Œ
results = await parallel_prompt.acall(query="ä½ å¥½ä¸–ç•Œ")
# results = ["Hello World", "ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ", "Bonjour le monde"]
```

### ParallelPrompt ç±»

```python
from alphora.prompter.parallel import ParallelPrompt

prompts = [
    BasePrompt(user_prompt="åˆ†ææƒ…æ„Ÿï¼š{{text}}"),
    BasePrompt(user_prompt="æå–å…³é”®è¯ï¼š{{text}}"),
    BasePrompt(user_prompt="ç”Ÿæˆæ‘˜è¦ï¼š{{text}}")
]

parallel = ParallelPrompt(prompts)

# åŒä¸€è¾“å…¥ï¼Œå¤šä¸ªåˆ†æ
results = await parallel.acall(text="è¿™æ˜¯ä¸€æ®µéœ€è¦åˆ†æçš„æ–‡æœ¬...")
sentiment, keywords, summary = results
```

### æ€§èƒ½ä¼˜åŠ¿

- å¹¶è¡Œæ‰§è¡Œå¤šä¸ª LLM è°ƒç”¨
- å‡å°‘æ€»ç­‰å¾…æ—¶é—´
- é€‚åˆå¤šè§’åº¦åˆ†æã€æ‰¹é‡ç¿»è¯‘ç­‰åœºæ™¯

---

## åå¤„ç†å™¨

### ä½¿ç”¨åå¤„ç†å™¨

```python
from alphora.postprocess.base_pp import BasePostProcessor

# è‡ªå®šä¹‰åå¤„ç†å™¨
class UpperCaseProcessor(BasePostProcessor):
    def __call__(self, generator):
        async for chunk in generator:
            chunk.content = chunk.content.upper()
            yield chunk

response = await prompt.acall(
    query="ä½ å¥½",
    is_stream=True,
    postprocessor=UpperCaseProcessor()
)
```

### å¤šä¸ªåå¤„ç†å™¨

```python
response = await prompt.acall(
    query="ç”Ÿæˆå†…å®¹",
    is_stream=True,
    postprocessor=[
        FilterProcessor(),
        FormatProcessor(),
        LogProcessor()
    ]
)
```

---

## JSON è¾“å‡º

### å¼ºåˆ¶ JSON æ ¼å¼

```python
response = await prompt.acall(
    query="åˆ—å‡ºä¸‰ä¸ªæ°´æœ",
    force_json=True  # è‡ªåŠ¨æ·»åŠ  JSON æŒ‡ä»¤å¹¶ä¿®å¤è¾“å‡º
)

# è‡ªåŠ¨ä½¿ç”¨ json_repair ä¿®å¤å¯èƒ½çš„æ ¼å¼é—®é¢˜
data = json.loads(response)
```

---

## API å‚è€ƒ

### BasePrompt

#### æ„é€ å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `user_prompt` | `str` | `None` | User æç¤ºè¯æ¨¡æ¿ |
| `template_path` | `str` | `None` | æ¨¡æ¿æ–‡ä»¶è·¯å¾„ |
| `system_prompt` | `str \| List[str]` | `None` | System æç¤ºè¯ |
| `verbose` | `bool` | `False` | è¯¦ç»†æ—¥å¿— |
| `callback` | `DataStreamer` | `None` | æµå¼å›è°ƒ |
| `content_type` | `str` | `'char'` | é»˜è®¤å†…å®¹ç±»å‹ |
| `agent_id` | `str` | `None` | å…³è”çš„ Agent ID |

#### æ–¹æ³•

| æ–¹æ³• | è¯´æ˜ |
|------|------|
| `add_llm(model)` | ç»‘å®š LLM å®ä¾‹ |
| `update_placeholder(**kwargs)` | æ›´æ–°æ¨¡æ¿å˜é‡ |
| `build_messages(query, force_json, runtime_system_prompt, history)` | æ„å»ºæ¶ˆæ¯åˆ—è¡¨ |
| `call(...)` | åŒæ­¥è°ƒç”¨ |
| `acall(...)` | å¼‚æ­¥è°ƒç”¨ |
| `render()` | æ¸²æŸ“ User æ¨¡æ¿ |

#### acall / call å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `query` | `str` | `None` | ç”¨æˆ·è¾“å…¥ |
| `is_stream` | `bool` | `False` | æ˜¯å¦æµå¼ |
| `tools` | `List` | `None` | å·¥å…·å®šä¹‰åˆ—è¡¨ |
| `multimodal_message` | `Message` | `None` | å¤šæ¨¡æ€æ¶ˆæ¯ |
| `return_generator` | `bool` | `False` | è¿”å›åŸå§‹ç”Ÿæˆå™¨ |
| `content_type` | `str` | `None` | è¦†ç›–å†…å®¹ç±»å‹ |
| `postprocessor` | `BasePostProcessor` | `None` | åå¤„ç†å™¨ |
| `enable_thinking` | `bool` | `False` | å¯ç”¨æ€è€ƒé“¾ |
| `force_json` | `bool` | `False` | å¼ºåˆ¶ JSON è¾“å‡º |
| `long_response` | `bool` | `False` | å¯ç”¨é•¿æ–‡æœ¬ç»­å†™ |
| `runtime_system_prompt` | `str \| List[str]` | `None` | è¿è¡Œæ—¶ç³»ç»Ÿæç¤º |
| `history` | `HistoryPayload` | `None` | å†å²è®°å½•è½½è· |

### PrompterOutput

ç»§æ‰¿è‡ª `str`ï¼Œå¯ç›´æ¥ä½œä¸ºå­—ç¬¦ä¸²ä½¿ç”¨ï¼ŒåŒæ—¶æä¾›é¢å¤–å±æ€§ï¼š

| å±æ€§ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `reasoning` | `str` | æ€è€ƒé“¾å†…å®¹ï¼ˆenable_thinking=True æ—¶ï¼‰ |
| `finish_reason` | `str` | ç»“æŸåŸå› ï¼ˆstop/length/tool_callsï¼‰ |
| `continuation_count` | `int` | ç»­å†™æ¬¡æ•°ï¼ˆlong_response=True æ—¶ï¼‰ |

### ToolCall

| å±æ€§/æ–¹æ³• | è¯´æ˜ |
|-----------|------|
| `tool_calls` | å·¥å…·è°ƒç”¨åˆ—è¡¨ |
| `content` | æ–‡æœ¬å†…å®¹ï¼ˆå¯èƒ½ä¸º Noneï¼‰ |
| `has_tool_calls` | æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ |
| `get_tool_names()` | è·å–æ‰€æœ‰å·¥å…·åç§° |
| `get_tool_call_ids()` | è·å–æ‰€æœ‰è°ƒç”¨ ID |
| `pretty_print()` | æ ¼å¼åŒ–æ‰“å° |
| `to_summary()` | ç”Ÿæˆå•è¡Œæ‘˜è¦ |

### ParallelPrompt

| æ–¹æ³• | è¯´æ˜ |
|------|------|
| `call(*args, **kwargs)` | åŒæ­¥å¹¶è¡Œæ‰§è¡Œ |
| `acall(*args, **kwargs)` | å¼‚æ­¥å¹¶è¡Œæ‰§è¡Œ |